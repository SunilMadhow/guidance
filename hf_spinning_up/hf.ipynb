{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a7d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /Users/sunilmadhow/anaconda3/bin/python\n",
      "sys.path[0..3]: ['/Users/sunilmadhow/Desktop/Desktop/guidance', '/Users/sunilmadhow/anaconda3/lib/python311.zip', '/Users/sunilmadhow/anaconda3/lib/python3.11'] ...\n",
      "BEFORE transformers.__version__: 4.32.1\n",
      "BEFORE transformers.__file__: /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages/transformers/__init__.py\n",
      "Removed: ['/Users/sunilmadhow/anaconda3/lib/python3.11/site-packages/transformers', '/Users/sunilmadhow/anaconda3/lib/python3.11/site-packages/transformers-4.57.1.dist-info']\n",
      "Collecting transformers>=4.44.2\n",
      "  Obtaining dependency information for transformers>=4.44.2 from https://files.pythonhosted.org/packages/71/d3/c16c3b3cf7655a67db1144da94b021c200ac1303f82428f2beef6c2e72bb/transformers-4.57.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers>=0.15.2 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.4.5 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (0.6.2)\n",
      "Requirement already satisfied: timm in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (1.0.20)\n",
      "Requirement already satisfied: filelock in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0) (1.1.10)\n",
      "Requirement already satisfied: torch in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from timm) (2.5.1)\n",
      "Requirement already satisfied: torchvision in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from timm) (0.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.44.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.44.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.44.2) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.44.2) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from jinja2->torch->timm) (2.1.1)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "Successfully installed transformers-4.57.1\n",
      "AFTER transformers.__version__: 4.57.1\n",
      "AFTER transformers.__file__: /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages/transformers/__init__.py\n",
      "SigLIP OK ✔\n"
     ]
    }
   ],
   "source": [
    "import sys, os, shutil, subprocess, glob, importlib, pkgutil\n",
    "\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"sys.path[0..3]:\", sys.path[:3], \"...\")\n",
    "\n",
    "# 1) Inspect current transformers (if any)\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"BEFORE transformers.__version__:\", transformers.__version__)\n",
    "    print(\"BEFORE transformers.__file__:\", transformers.__file__)\n",
    "except Exception as e:\n",
    "    print(\"BEFORE transformers: not importable ->\", e)\n",
    "\n",
    "# 2) Remove every copy of transformers visible on sys.path (package dir + dist-info)\n",
    "removed = []\n",
    "for p in list(sys.path):\n",
    "    if not os.path.isdir(p):\n",
    "        continue\n",
    "    pkg_dir = os.path.join(p, \"transformers\")\n",
    "    if os.path.isdir(pkg_dir):\n",
    "        shutil.rmtree(pkg_dir, ignore_errors=True)\n",
    "        removed.append(pkg_dir)\n",
    "    for di in glob.glob(os.path.join(p, \"transformers-*.dist-info\")):\n",
    "        shutil.rmtree(di, ignore_errors=True)\n",
    "        removed.append(di)\n",
    "print(\"Removed:\", removed if removed else \"(none found)\")\n",
    "\n",
    "# 3) Hard-reinstall a compatible stack into THIS interpreter\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"--no-cache-dir\",\n",
    "    \"transformers>=4.44.2\", \"tokenizers>=0.15.2\",\n",
    "    \"huggingface-hub>=0.24.0\", \"safetensors>=0.4.5\", \"timm\"\n",
    "])\n",
    "\n",
    "# 4) Clear any cached module, then verify the symbol exists\n",
    "for name in list(sys.modules):\n",
    "    if name.startswith(\"transformers\"):\n",
    "        del sys.modules[name]\n",
    "\n",
    "import importlib\n",
    "transformers = importlib.import_module(\"transformers\")\n",
    "print(\"AFTER transformers.__version__:\", transformers.__version__)\n",
    "print(\"AFTER transformers.__file__:\", transformers.__file__)\n",
    "\n",
    "from transformers import SiglipImageProcessor\n",
    "print(\"SigLIP OK ✔\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df872acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: /Users/sunilmadhow/anaconda3/bin/python\n",
      "transformers: 4.32.1\n",
      "file: /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages/transformers/__init__.py\n",
      "['/Users/sunilmadhow/anaconda3/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys, pkgutil, importlib\n",
    "print(\"PY:\", sys.executable)\n",
    "\n",
    "import transformers\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"file:\", transformers.__file__)\n",
    "\n",
    "# sanity: where would pip install to?\n",
    "import site, pprint\n",
    "pprint.pprint(site.getsitepackages() if hasattr(site, \"getsitepackages\") else site.getusersitepackages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd184f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.57.1\n",
      "Uninstalling transformers-4.57.1:\n",
      "  Successfully uninstalled transformers-4.57.1\n",
      "Found existing installation: tokenizers 0.22.1\n",
      "Uninstalling tokenizers-0.22.1:\n",
      "  Successfully uninstalled tokenizers-0.22.1\n",
      "Collecting transformers>=4.44.2\n",
      "  Obtaining dependency information for transformers>=4.44.2 from https://files.pythonhosted.org/packages/71/d3/c16c3b3cf7655a67db1144da94b021c200ac1303f82428f2beef6c2e72bb/transformers-4.57.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers>=0.15.2\n",
      "  Obtaining dependency information for tokenizers>=0.15.2 from https://files.pythonhosted.org/packages/1c/58/2aa8c874d02b974990e89ff95826a4852a8b2a273c7d1b4411cdd45a4565/tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.4.5 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (0.6.2)\n",
      "Requirement already satisfied: timm in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (1.0.20)\n",
      "Requirement already satisfied: filelock in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from transformers>=4.44.2) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0) (1.1.10)\n",
      "Requirement already satisfied: torch in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from timm) (2.5.1)\n",
      "Requirement already satisfied: torchvision in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from timm) (0.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.44.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.44.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.44.2) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.44.2) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (from jinja2->torch->timm) (2.1.1)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"transformers\", \"tokenizers\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"--no-cache-dir\",\n",
    "    \"transformers>=4.44.2\", \"tokenizers>=0.15.2\",\n",
    "    \"huggingface-hub>=0.24.0\", \"safetensors>=0.4.5\", \"timm\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ef25af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusers: 0.35.2\n",
      "transformers: 4.32.1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SiglipImageProcessor' from 'transformers' (/Users/sunilmadhow/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiffusers:\u001b[39m\u001b[38;5;124m\"\u001b[39m, diffusers\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers:\u001b[39m\u001b[38;5;124m\"\u001b[39m, transformers\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SiglipImageProcessor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigLIP OK ✔\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SiglipImageProcessor' from 'transformers' (/Users/sunilmadhow/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import diffusers, transformers\n",
    "print(\"diffusers:\", diffusers.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "from transformers import SiglipImageProcessor\n",
    "print(\"SigLIP OK ✔\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da75d6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusers: 0.35.2\n",
      "transformers: 4.57.1\n",
      "SigLIP OK: True\n"
     ]
    }
   ],
   "source": [
    "import diffusers, transformers\n",
    "from transformers import SiglipImageProcessor\n",
    "print(\"diffusers:\", diffusers.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"SigLIP OK:\", SiglipImageProcessor is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29309d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in /Users/sunilmadhow/anaconda3/lib/python3.11/site-packages (6.33.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bccd10a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acb22ed2f4945408f9263dc272225bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/543 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af34e851b771424291ddb8411e76905b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'resume_download': False} are not expected by StableDiffusionPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75abfff9156340f483d92b6932c2b47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The MPS backend is supported on MacOS 12.3+.Current OS version can be queried using `sw_vers`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-2-1-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m DiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     model_id,\n\u001b[1;32m      8\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m      9\u001b[0m     use_safetensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,   \u001b[38;5;66;03m# ensures full re-download\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     force_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m      \u001b[38;5;66;03m# overwrites any leftovers\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m pipe\u001b[38;5;241m.\u001b[39menable_attention_slicing()\n\u001b[1;32m     15\u001b[0m img \u001b[38;5;241m=\u001b[39m pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma cozy cabin in a snowy forest at dusk, high detail\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:541\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_group_offloaded:\n\u001b[0;32m--> 541\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device, dtype)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    544\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[1;32m    548\u001b[0m ):\n\u001b[1;32m    549\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    555\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:4343\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   4339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4340\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4341\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4342\u001b[0m         )\n\u001b[0;32m-> 4343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1327\u001b[0m         device,\n\u001b[1;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1329\u001b[0m         non_blocking,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The MPS backend is supported on MacOS 12.3+.Current OS version can be queried using `sw_vers`"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    resume_download=False,   # ensures full re-download\n",
    "    force_download=True      # overwrites any leftovers\n",
    ").to(\"mps\")\n",
    "\n",
    "pipe.enable_attention_slicing()\n",
    "img = pipe(\"a cozy cabin in a snowy forest at dusk, high detail\").images[0]\n",
    "img.save(\"result.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e062d7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43811082ed94f94b4a69f8790a279f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a112030c5504ece86904dae95d199f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "import os\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"mps\" else torch.float32\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "pipe.enable_attention_slicing()  # harmless on CPU too\n",
    "\n",
    "# Generate 10 images\n",
    "images = pipe(\n",
    "    \"A hand\",\n",
    "    num_inference_steps=25,      # keep steps low on CPU\n",
    "    num_images_per_prompt=10\n",
    ").images\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    img.save(f\"results/result_{i+1}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca71ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vae': AutoencoderKL(\n",
       "   (encoder): Encoder(\n",
       "     (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (down_blocks): ModuleList(\n",
       "       (0): DownEncoderBlock2D(\n",
       "         (resnets): ModuleList(\n",
       "           (0-1): 2 x ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "           )\n",
       "         )\n",
       "         (downsamplers): ModuleList(\n",
       "           (0): Downsample2D(\n",
       "             (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): DownEncoderBlock2D(\n",
       "         (resnets): ModuleList(\n",
       "           (0): ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "             (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "           )\n",
       "           (1): ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "           )\n",
       "         )\n",
       "         (downsamplers): ModuleList(\n",
       "           (0): Downsample2D(\n",
       "             (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (2): DownEncoderBlock2D(\n",
       "         (resnets): ModuleList(\n",
       "           (0): ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "             (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "           )\n",
       "           (1): ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "           )\n",
       "         )\n",
       "         (downsamplers): ModuleList(\n",
       "           (0): Downsample2D(\n",
       "             (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (3): DownEncoderBlock2D(\n",
       "         (resnets): ModuleList(\n",
       "           (0-1): 2 x ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (mid_block): UNetMidBlock2D(\n",
       "       (attentions): ModuleList(\n",
       "         (0): Attention(\n",
       "           (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "           (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (to_out): ModuleList(\n",
       "             (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (1): Dropout(p=0.0, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (resnets): ModuleList(\n",
       "         (0-1): 2 x ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "           (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "     (conv_act): SiLU()\n",
       "     (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   )\n",
       "   (decoder): Decoder(\n",
       "     (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (up_blocks): ModuleList(\n",
       "       (0-1): 2 x UpDecoderBlock2D(\n",
       "         (resnets): ModuleList(\n",
       "           (0-2): 3 x ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "           )\n",
       "         )\n",
       "         (upsamplers): ModuleList(\n",
       "           (0): Upsample2D(\n",
       "             (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (2): UpDecoderBlock2D(\n",
       "         (resnets): ModuleList(\n",
       "           (0): ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "             (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "           )\n",
       "           (1-2): 2 x ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "           )\n",
       "         )\n",
       "         (upsamplers): ModuleList(\n",
       "           (0): Upsample2D(\n",
       "             (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (3): UpDecoderBlock2D(\n",
       "         (resnets): ModuleList(\n",
       "           (0): ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "             (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "           )\n",
       "           (1-2): 2 x ResnetBlock2D(\n",
       "             (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "             (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "             (nonlinearity): SiLU()\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (mid_block): UNetMidBlock2D(\n",
       "       (attentions): ModuleList(\n",
       "         (0): Attention(\n",
       "           (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "           (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (to_out): ModuleList(\n",
       "             (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (1): Dropout(p=0.0, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (resnets): ModuleList(\n",
       "         (0-1): 2 x ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "           (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "     (conv_act): SiLU()\n",
       "     (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   )\n",
       "   (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "   (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       " ),\n",
       " 'text_encoder': CLIPTextModel(\n",
       "   (text_model): CLIPTextTransformer(\n",
       "     (embeddings): CLIPTextEmbeddings(\n",
       "       (token_embedding): Embedding(49408, 1024)\n",
       "       (position_embedding): Embedding(77, 1024)\n",
       "     )\n",
       "     (encoder): CLIPEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-22): 23 x CLIPEncoderLayer(\n",
       "           (self_attn): CLIPAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): CLIPMLP(\n",
       "             (activation_fn): GELUActivation()\n",
       "             (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "             (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           )\n",
       "           (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       " ),\n",
       " 'tokenizer': CLIPTokenizer(name_or_path='/Users/sunilmadhow/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '!'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t0: AddedToken(\"!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " }\n",
       " ),\n",
       " 'unet': UNet2DConditionModel(\n",
       "   (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (time_proj): Timesteps()\n",
       "   (time_embedding): TimestepEmbedding(\n",
       "     (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "     (act): SiLU()\n",
       "     (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "   )\n",
       "   (down_blocks): ModuleList(\n",
       "     (0): CrossAttnDownBlock2D(\n",
       "       (attentions): ModuleList(\n",
       "         (0-1): 2 x Transformer2DModel(\n",
       "           (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "           (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "           (transformer_blocks): ModuleList(\n",
       "             (0): BasicTransformerBlock(\n",
       "               (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn1): Attention(\n",
       "                 (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                 (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                 (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn2): Attention(\n",
       "                 (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                 (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                 (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "               (ff): FeedForward(\n",
       "                 (net): ModuleList(\n",
       "                   (0): GEGLU(\n",
       "                     (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                   )\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                   (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "         )\n",
       "       )\n",
       "       (resnets): ModuleList(\n",
       "         (0-1): 2 x ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "           (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "         )\n",
       "       )\n",
       "       (downsamplers): ModuleList(\n",
       "         (0): Downsample2D(\n",
       "           (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): CrossAttnDownBlock2D(\n",
       "       (attentions): ModuleList(\n",
       "         (0-1): 2 x Transformer2DModel(\n",
       "           (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "           (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "           (transformer_blocks): ModuleList(\n",
       "             (0): BasicTransformerBlock(\n",
       "               (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn1): Attention(\n",
       "                 (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                 (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                 (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn2): Attention(\n",
       "                 (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                 (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                 (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "               (ff): FeedForward(\n",
       "                 (net): ModuleList(\n",
       "                   (0): GEGLU(\n",
       "                     (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                   )\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                   (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "         )\n",
       "       )\n",
       "       (resnets): ModuleList(\n",
       "         (0): ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "           (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (1): ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "           (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "         )\n",
       "       )\n",
       "       (downsamplers): ModuleList(\n",
       "         (0): Downsample2D(\n",
       "           (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (2): CrossAttnDownBlock2D(\n",
       "       (attentions): ModuleList(\n",
       "         (0-1): 2 x Transformer2DModel(\n",
       "           (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "           (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "           (transformer_blocks): ModuleList(\n",
       "             (0): BasicTransformerBlock(\n",
       "               (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn1): Attention(\n",
       "                 (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                 (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                 (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn2): Attention(\n",
       "                 (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                 (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                 (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "               (ff): FeedForward(\n",
       "                 (net): ModuleList(\n",
       "                   (0): GEGLU(\n",
       "                     (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                   )\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                   (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         )\n",
       "       )\n",
       "       (resnets): ModuleList(\n",
       "         (0): ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "           (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (1): ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "           (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "         )\n",
       "       )\n",
       "       (downsamplers): ModuleList(\n",
       "         (0): Downsample2D(\n",
       "           (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (3): DownBlock2D(\n",
       "       (resnets): ModuleList(\n",
       "         (0-1): 2 x ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "           (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (up_blocks): ModuleList(\n",
       "     (0): UpBlock2D(\n",
       "       (resnets): ModuleList(\n",
       "         (0-2): 3 x ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "           (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "       )\n",
       "       (upsamplers): ModuleList(\n",
       "         (0): Upsample2D(\n",
       "           (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): CrossAttnUpBlock2D(\n",
       "       (attentions): ModuleList(\n",
       "         (0-2): 3 x Transformer2DModel(\n",
       "           (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "           (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "           (transformer_blocks): ModuleList(\n",
       "             (0): BasicTransformerBlock(\n",
       "               (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn1): Attention(\n",
       "                 (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                 (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                 (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn2): Attention(\n",
       "                 (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                 (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                 (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "               (ff): FeedForward(\n",
       "                 (net): ModuleList(\n",
       "                   (0): GEGLU(\n",
       "                     (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                   )\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                   (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         )\n",
       "       )\n",
       "       (resnets): ModuleList(\n",
       "         (0-1): 2 x ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "           (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (2): ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "           (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "       )\n",
       "       (upsamplers): ModuleList(\n",
       "         (0): Upsample2D(\n",
       "           (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (2): CrossAttnUpBlock2D(\n",
       "       (attentions): ModuleList(\n",
       "         (0-2): 3 x Transformer2DModel(\n",
       "           (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "           (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "           (transformer_blocks): ModuleList(\n",
       "             (0): BasicTransformerBlock(\n",
       "               (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn1): Attention(\n",
       "                 (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                 (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                 (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn2): Attention(\n",
       "                 (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                 (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                 (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "               (ff): FeedForward(\n",
       "                 (net): ModuleList(\n",
       "                   (0): GEGLU(\n",
       "                     (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                   )\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                   (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "         )\n",
       "       )\n",
       "       (resnets): ModuleList(\n",
       "         (0): ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "           (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (1): ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "           (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (2): ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "           (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "       )\n",
       "       (upsamplers): ModuleList(\n",
       "         (0): Upsample2D(\n",
       "           (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (3): CrossAttnUpBlock2D(\n",
       "       (attentions): ModuleList(\n",
       "         (0-2): 3 x Transformer2DModel(\n",
       "           (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "           (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "           (transformer_blocks): ModuleList(\n",
       "             (0): BasicTransformerBlock(\n",
       "               (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn1): Attention(\n",
       "                 (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                 (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                 (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn2): Attention(\n",
       "                 (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                 (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                 (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                 (to_out): ModuleList(\n",
       "                   (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                 )\n",
       "               )\n",
       "               (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "               (ff): FeedForward(\n",
       "                 (net): ModuleList(\n",
       "                   (0): GEGLU(\n",
       "                     (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                   )\n",
       "                   (1): Dropout(p=0.0, inplace=False)\n",
       "                   (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "         )\n",
       "       )\n",
       "       (resnets): ModuleList(\n",
       "         (0): ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "           (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "         (1-2): 2 x ResnetBlock2D(\n",
       "           (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "           (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "           (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "           (nonlinearity): SiLU()\n",
       "           (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (mid_block): UNetMidBlock2DCrossAttn(\n",
       "     (attentions): ModuleList(\n",
       "       (0): Transformer2DModel(\n",
       "         (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "         (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         (transformer_blocks): ModuleList(\n",
       "           (0): BasicTransformerBlock(\n",
       "             (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "             (attn1): Attention(\n",
       "               (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "               (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "               (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "               (to_out): ModuleList(\n",
       "                 (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                 (1): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "             (attn2): Attention(\n",
       "               (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "               (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "               (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "               (to_out): ModuleList(\n",
       "                 (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                 (1): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "             (ff): FeedForward(\n",
       "               (net): ModuleList(\n",
       "                 (0): GEGLU(\n",
       "                   (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                 )\n",
       "                 (1): Dropout(p=0.0, inplace=False)\n",
       "                 (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (resnets): ModuleList(\n",
       "       (0-1): 2 x ResnetBlock2D(\n",
       "         (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "         (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (nonlinearity): SiLU()\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "   (conv_act): SiLU()\n",
       "   (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       " ),\n",
       " 'scheduler': PNDMScheduler {\n",
       "   \"_class_name\": \"PNDMScheduler\",\n",
       "   \"_diffusers_version\": \"0.35.2\",\n",
       "   \"beta_end\": 0.012,\n",
       "   \"beta_schedule\": \"scaled_linear\",\n",
       "   \"beta_start\": 0.00085,\n",
       "   \"clip_sample\": false,\n",
       "   \"num_train_timesteps\": 1000,\n",
       "   \"prediction_type\": \"epsilon\",\n",
       "   \"set_alpha_to_one\": false,\n",
       "   \"skip_prk_steps\": true,\n",
       "   \"steps_offset\": 1,\n",
       "   \"timestep_spacing\": \"leading\",\n",
       "   \"trained_betas\": null\n",
       " },\n",
       " 'safety_checker': None,\n",
       " 'feature_extractor': CLIPImageProcessor {\n",
       "   \"crop_size\": {\n",
       "     \"height\": 224,\n",
       "     \"width\": 224\n",
       "   },\n",
       "   \"do_center_crop\": true,\n",
       "   \"do_convert_rgb\": true,\n",
       "   \"do_normalize\": true,\n",
       "   \"do_rescale\": true,\n",
       "   \"do_resize\": true,\n",
       "   \"image_mean\": [\n",
       "     0.48145466,\n",
       "     0.4578275,\n",
       "     0.40821073\n",
       "   ],\n",
       "   \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "   \"image_std\": [\n",
       "     0.26862954,\n",
       "     0.26130258,\n",
       "     0.27577711\n",
       "   ],\n",
       "   \"resample\": 3,\n",
       "   \"rescale_factor\": 0.00392156862745098,\n",
       "   \"size\": {\n",
       "     \"shortest_edge\": 224\n",
       "   }\n",
       " },\n",
       " 'image_encoder': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d6d8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0144bddaeb144749ab8c531866219bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"mps\" else torch.float32\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "pipe.enable_attention_slicing()  # harmless on CPU too\n",
    "\n",
    "# img = pipe(\n",
    "#     \"a crying programmer\",\n",
    "#     num_inference_steps=25,      # keep steps low on CPU\n",
    "# ).images[0]\n",
    "# img.save(\"result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "128efc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision.utils as tv\n",
    "\n",
    "device = \"cpu\"   # stay on CPU for your macOS 12.0.1\n",
    "pipe.unet.to(dtype=torch.float32)\n",
    "pipe.text_encoder.to(dtype=torch.float32)\n",
    "pipe.vae.to(dtype=torch.float32)\n",
    "\n",
    "prompt = \"A crying programmer\"\n",
    "neg_prompt = \"\"          # empty negative is fine\n",
    "guidance_scale = 2 # 7.5\n",
    "num_steps = 30\n",
    "H = W = 512\n",
    "\n",
    "# 1) Use the pipeline's PUBLIC encoder (handles masks/shapes correctly)\n",
    "try:\n",
    "    # Newer diffusers exposes this\n",
    "    pos_emb, neg_emb = pipe.encode_prompt(\n",
    "        prompt=prompt,\n",
    "        device=device,\n",
    "        num_images_per_prompt=1,\n",
    "        do_classifier_free_guidance=True,\n",
    "        negative_prompt=neg_prompt,\n",
    "    )\n",
    "except AttributeError:\n",
    "    # Fallback: manual encode (older versions). This may be slightly weaker on some pipelines.\n",
    "    def encode(text):\n",
    "        tok = pipe.tokenizer\n",
    "        toks = tok([text], padding=\"max_length\",\n",
    "                   max_length=tok.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        toks = {k: v.to(device) for k, v in toks.items()}\n",
    "        with torch.inference_mode():\n",
    "            return pipe.text_encoder(**toks).last_hidden_state.to(torch.float32)\n",
    "    pos_emb, neg_emb = encode(prompt), encode(neg_prompt)\n",
    "\n",
    "# 2) Timesteps\n",
    "sch = pipe.scheduler\n",
    "sch.set_timesteps(num_steps)\n",
    "\n",
    "# 3) Latent init EXACTLY like the pipeline does (scale by init_noise_sigma)\n",
    "latent = torch.randn(1, 4, H//8, W//8, device=device, dtype=torch.float32)\n",
    "if hasattr(sch, \"init_noise_sigma\"):\n",
    "    latent = latent * sch.init_noise_sigma\n",
    "\n",
    "# # 4) Step loop with CFG (scheduler-agnostic: scale_model_input → unet → step)\n",
    "# with torch.inference_mode():\n",
    "#     for t in sch.timesteps:\n",
    "#         x_in = sch.scale_model_input(latent, t)\n",
    "#         # duplicate for CFG, run once\n",
    "#         x_cat  = torch.cat([x_in, x_in], dim=0)\n",
    "#         e_cat  = torch.cat([neg_emb, pos_emb], dim=0)\n",
    "#         pred   = pipe.unet(x_cat, t, encoder_hidden_states=e_cat).sample\n",
    "#         eps_neg, eps_pos = pred.chunk(2, dim=0)\n",
    "#         eps = eps_neg + guidance_scale * (eps_pos - eps_neg)\n",
    "#         latent = sch.step(eps, t, latent).prev_sample\n",
    "\n",
    "#     # 5) Decode using the VAE scaling_factor from the pipeline config\n",
    "#     scale = getattr(pipe.vae.config, \"scaling_factor\", 0.18215)\n",
    "#     img = pipe.vae.decode(latent / scale).sample          # [-1,1]\n",
    "#     img = (img.clamp(-1,1) + 1) / 2                       # [0,1]\n",
    "#     tv.save_image(img, \"result_manual_matches_pipe.png\")\n",
    "\n",
    "# print(\"Saved result_manual_matches_pipe.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6234ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd_diffusion_api.py\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Diffusion:\n",
    "    \"\"\"\n",
    "    Stable Diffusion-backed adapter exposing the same API as your toy Diffusion:\n",
    "        - calc_samples()\n",
    "        - single_step(t, X)\n",
    "        - single_step_iid_copies(t, X, size=1)\n",
    "        - single_sample(t, x)\n",
    "\n",
    "    Setup (once):\n",
    "        d = Diffusion(S=...)             # ONLY S at construction\n",
    "        d.set_prompt(\"A crying programmer\", negative_prompt=\"\")\n",
    "        d.set_steps(30)\n",
    "        d.set_size(512, 512)             # optional; default 512x512\n",
    "\n",
    "    Notes:\n",
    "      * Uses the global `pipe` already loaded in your notebook.\n",
    "      * NumPy in/out shapes match your original class.\n",
    "      * Keeps CPU/fp32 for stability on macOS 12.0.1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=10000):\n",
    "        # config\n",
    "        self.S = int(S)\n",
    "        self.device = torch.device(\"cpu\")  # macOS 12.0.1: stay on CPU\n",
    "        # pull modules from the global pipeline\n",
    "        self.pipe = globals()[\"pipe\"]      # expect 'pipe' to be defined already\n",
    "        self.pipe.unet.to(dtype=torch.float32, device=self.device)\n",
    "        self.pipe.text_encoder.to(dtype=torch.float32, device=self.device)\n",
    "        self.pipe.vae.to(dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.unet = self.pipe.unet\n",
    "        self.vae = self.pipe.vae\n",
    "        self.scheduler = self.pipe.scheduler\n",
    "        self.scale = getattr(self.vae.config, \"scaling_factor\", 0.18215)\n",
    "\n",
    "        # runtime state\n",
    "        self.timesteps = None  # list of scheduler-native t's\n",
    "        self.T = None\n",
    "        self.H = 512\n",
    "        self.W = 512\n",
    "        self.C = self.unet.in_channels  # 4\n",
    "        self.Hp = self.H // 8\n",
    "        self.Wp = self.W // 8\n",
    "        self.d = self.C * self.Hp * self.Wp\n",
    "\n",
    "        # embeddings for CFG (must be set via set_prompt)\n",
    "        self._pos = None\n",
    "        self._neg = None\n",
    "        self.guidance_scale = 7.5\n",
    "\n",
    "        # tiny jitter to make iid proposals differ even under deterministic schedulers\n",
    "        self.proposal_jitter = 0.0  # set >0 (e.g., 1e-4) if you need diversity\n",
    "\n",
    "    # ---------- configuration ----------\n",
    "    def set_prompt(self, prompt: str, negative_prompt: str = \"\"):\n",
    "        \"\"\"Encode prompt the pipeline's way (public API if available).\"\"\"\n",
    "        try:\n",
    "            pos, neg = self.pipe.encode_prompt(\n",
    "                prompt=prompt,\n",
    "                device=self.device,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "        except AttributeError:\n",
    "            # fallback: manual encode\n",
    "            tok = self.pipe.tokenizer\n",
    "            def _enc(text):\n",
    "                toks = tok([text], padding=\"max_length\",\n",
    "                           max_length=tok.model_max_length, truncation=True,\n",
    "                           return_tensors=\"pt\")\n",
    "                toks = {k: v.to(self.device) for k, v in toks.items()}\n",
    "                with torch.inference_mode():\n",
    "                    return self.pipe.text_encoder(**toks).last_hidden_state\n",
    "            pos, neg = _enc(prompt), _enc(negative_prompt)\n",
    "\n",
    "        dt = self.unet.dtype\n",
    "        self._pos = pos.to(self.device, dt)\n",
    "        self._neg = neg.to(self.device, dt)\n",
    "\n",
    "    def set_steps(self, num_inference_steps: int):\n",
    "        \"\"\"Define the reverse-time grid via the scheduler.\"\"\"\n",
    "        self.scheduler.set_timesteps(int(num_inference_steps))\n",
    "        self.timesteps = list(self.scheduler.timesteps)\n",
    "        self.T = len(self.timesteps)\n",
    "        return self.T\n",
    "\n",
    "    def set_size(self, H: int, W: int):\n",
    "        \"\"\"Set output resolution (affects latent shape & d).\"\"\"\n",
    "        self.H, self.W = int(H), int(W)\n",
    "        self.Hp, self.Wp = self.H // 8, self.W // 8\n",
    "        self.d = self.C * self.Hp * self.Wp\n",
    "        return self.d\n",
    "\n",
    "    def set_guidance(self, scale: float):\n",
    "        self.guidance_scale = float(scale)\n",
    "\n",
    "    import torch\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def final_image(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Decode latent(s) into pixel-space images in [0,1].\n",
    "        Input:\n",
    "            X: (K, d) — latent vectors (e.g., the x_0 outputs from calc_samples)\n",
    "        Returns:\n",
    "            imgs: torch.Tensor of shape (K, 3, H, W), in [0,1]\n",
    "        \"\"\"\n",
    "        # reshape numpy vectors back to latent maps\n",
    "        z = torch.from_numpy(X.astype(np.float32)).to(self.device, self.unet.dtype)\n",
    "        z = z.reshape(-1, self.C, self.Hp, self.Wp)\n",
    "    \n",
    "        img = self.vae.decode(z / self.scale).sample  # [-1,1]\n",
    "        img = (img.clamp(-1, 1) + 1) / 2              # [0,1]\n",
    "        return img\n",
    "\n",
    "\n",
    "    # ---------- core helpers (torch) ----------\n",
    "    @torch.inference_mode()\n",
    "    def _step_torch(self, t_idx: int, x_tp1_lat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"One reverse step with CFG. x_tp1_lat: (B,4,H',W'). returns x_t (same shape).\"\"\"\n",
    "        assert self._pos is not None and self._neg is not None, \"call set_prompt() first\"\n",
    "        t = self.timesteps[t_idx]\n",
    "\n",
    "        x_in = self.scheduler.scale_model_input(x_tp1_lat, t)\n",
    "        # concat for CFG\n",
    "        x_cat = torch.cat([x_in, x_in], dim=0)\n",
    "        e_cat = torch.cat([self._neg, self._pos], dim=0)\n",
    "        pred  = self.unet(x_cat, t, encoder_hidden_states=e_cat).sample\n",
    "        eps_neg, eps_pos = pred.chunk(2, dim=0)\n",
    "        eps = eps_neg + self.guidance_scale * (eps_pos - eps_neg)\n",
    "        out = self.scheduler.step(eps, t, x_tp1_lat)\n",
    "        return out.prev_sample  # x_t\n",
    "\n",
    "    def _vec_to_lat(self, X_np: np.ndarray) -> torch.Tensor:\n",
    "        return torch.from_numpy(X_np.astype(np.float32)).to(self.device, self.unet.dtype)\\\n",
    "                     .reshape(-1, self.C, self.Hp, self.Wp)\n",
    "\n",
    "    def _lat_to_vec(self, Z: torch.Tensor) -> np.ndarray:\n",
    "        return Z.detach().to(dtype=torch.float32).cpu().numpy().reshape(Z.shape[0], -1)\n",
    "\n",
    "    def _init_noise(self, K: int) -> torch.Tensor:\n",
    "        lat = torch.randn(K, self.C, self.Hp, self.Wp, device=self.device, dtype=self.unet.dtype)\n",
    "        if hasattr(self.scheduler, \"init_noise_sigma\"):\n",
    "            lat = lat * self.scheduler.init_noise_sigma\n",
    "        return lat\n",
    "\n",
    "    # ---------- API: calc_samples (like your toy class) ----------\n",
    "    def calc_samples(self) -> np.ndarray:\n",
    "        \"\"\"Return (S, T, d) in chronological order (x_0..x_{T-1}).\"\"\"\n",
    "        assert self.T is not None and self.timesteps is not None, \"call set_steps(n) first\"\n",
    "        assert self._pos is not None, \"call set_prompt() first\"\n",
    "\n",
    "        # (S, T, d)\n",
    "        X = np.zeros((self.S, self.T, self.d), dtype=np.float32)\n",
    "\n",
    "        # start at x_T (last index in reverse-time array)\n",
    "        XT = self._init_noise(self.S)  # (S,4,H',W')\n",
    "        X[:, -1, :] = self._lat_to_vec(XT)\n",
    "\n",
    "        # reverse diffusion: fill t=T-2 .. 0\n",
    "        with torch.inference_mode():\n",
    "            x_lat = XT\n",
    "            for t in range(self.T - 2, -1, -1):\n",
    "                x_lat = self._step_torch(t, x_lat)\n",
    "                X[:, t, :] = self._lat_to_vec(x_lat)\n",
    "\n",
    "        # flip to chronological order (x_0..x_{T-1})\n",
    "        return np.flip(X, axis=1)\n",
    "\n",
    "    # ---------- API: single_step ----------\n",
    "    def single_step(self, t: int, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        X: (K, d) representing x_{t+1} for K chains\n",
    "        Return: (K, d) with x_t\n",
    "        \"\"\"\n",
    "        assert X.ndim == 2 and X.shape[1] == self.d, \"X must be (K, d)\"\n",
    "        x_lat = self._vec_to_lat(X)                 # (K,4,H',W')\n",
    "        with torch.inference_mode():\n",
    "            x_t = self._step_torch(t, x_lat)        # (K,4,H',W')\n",
    "        return self._lat_to_vec(x_t)                # (K, d)\n",
    "\n",
    "    # ---------- API: single_step_iid_copies ----------\n",
    "    def single_step_iid_copies(self, t: int, X: np.ndarray, size: int = 1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return: (size, K, d) proposals for each prefix x_{t+1} in X.\n",
    "        If the current scheduler is deterministic, results would be identical across copies;\n",
    "        set self.proposal_jitter > 0 to inject tiny pre-step noise and make them iid.\n",
    "        \"\"\"\n",
    "        assert X.ndim == 2 and X.shape[1] == self.d, \"X must be (K, d)\"\n",
    "        K = X.shape[0]\n",
    "        x_tp1 = self._vec_to_lat(X)  # (K,4,H',W')\n",
    "\n",
    "        props = []\n",
    "        with torch.inference_mode():\n",
    "            for _ in range(int(size)):\n",
    "                x_in = x_tp1\n",
    "                if self.proposal_jitter > 0:\n",
    "                    x_in = x_in + self.proposal_jitter * torch.randn_like(x_in)\n",
    "                x_t = self._step_torch(t, x_in)   # (K,4,H',W')\n",
    "                props.append(self._lat_to_vec(x_t))  # (K, d)\n",
    "        return np.stack(props, axis=0)  # (size, K, d)\n",
    "\n",
    "    # ---------- API: single_sample ----------\n",
    "    def single_sample(self, t: int, x: np.ndarray, size: int = 1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        x: (d,) vector (single chain's x_{t+1})\n",
    "        Return: (d,) vector x_t   (size arg kept for API parity; one sample returned)\n",
    "        \"\"\"\n",
    "        assert x.ndim == 1 and x.shape[0] == self.d\n",
    "        X = x.reshape(1, -1)\n",
    "        out = self.single_step(t, X)  # (1, d)\n",
    "        return out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "843ccd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zk/bk5jkrk11398bfvxps9gbhw00000gn/T/ipykernel_67783/4287216858.py:45: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  self.C = self.unet.in_channels  # 4\n"
     ]
    }
   ],
   "source": [
    "diff = Diffusion(S = 1)\n",
    "diff.set_steps(30)\n",
    "diff.set_prompt(\"A crying programmer sitting at a desk\", negative_prompt=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27bb7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diff.calc_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f76b099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = diff.final_image(X[:, 0, :]) # (1,3,H,W) in [0,1]\n",
    "tv.save_image(img, \"result_diffusion_api.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14899ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43811082ed94f94b4a69f8790a279f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a112030c5504ece86904dae95d199f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "import os\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"mps\" else torch.float32\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "pipe.enable_attention_slicing()  # harmless on CPU too\n",
    "\n",
    "# Generate 10 images\n",
    "images = pipe(\n",
    "    \"A hand\",\n",
    "    num_inference_steps=25,      # keep steps low on CPU\n",
    "    num_images_per_prompt=10\n",
    ").images\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    img.save(f\"results/result_{i+1}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd0d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imagereward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimagereward\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageReward\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m rm \u001b[38;5;241m=\u001b[39m ImageReward(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageReward-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imagereward'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901b359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e70e270fba4413a2519925a088f8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/556 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a526da53d3e84724924de4408169b587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8243fcbe7dc74afb8d1709ed6c6c65f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511c7edfe18e46d2aa83476b01a171fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efab68ee2a8452cadf9d8e118784bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f783ae0d1ffb4923aee525cb710452e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468df23ad1384054aed5b13ac5213f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a7c8bdc41e4e389810508332395daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PickScore: 20.692415237426758\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "proc = AutoProcessor.from_pretrained(\"yuvalkirstain/PickScore_v1\")\n",
    "model = AutoModel.from_pretrained(\"yuvalkirstain/PickScore_v1\")\n",
    "img = Image.open(\"result_000005.png\").convert(\"RGB\")\n",
    "inputs = proc(text=[\"a hand\"], images=[img], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    score = model(**inputs).logits_per_image.squeeze().item()\n",
    "print(\"PickScore:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecbfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore:  10%|█         | 1/10 [00:08<01:20,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_1.png: PickScore=20.3901, Adjusted=1.9503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore:  20%|██        | 2/10 [00:15<00:58,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_2.png: PickScore=22.1450, Adjusted=10.7251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore:  30%|███       | 3/10 [00:21<00:48,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_3.png: PickScore=20.0081, Adjusted=0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore:  40%|████      | 4/10 [00:22<00:27,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_4.png: PickScore=21.5309, Adjusted=7.6546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore:  50%|█████     | 5/10 [00:23<00:15,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_5.png: PickScore=19.4534, Adjusted=-2.7331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore:  60%|██████    | 6/10 [00:23<00:09,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_6.png: PickScore=21.1009, Adjusted=5.5044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore:  70%|███████   | 7/10 [00:26<00:07,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_7.png: PickScore=20.0444, Adjusted=0.2219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore:  80%|████████  | 8/10 [00:37<00:10,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_8.png: PickScore=20.6095, Adjusted=3.0477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore:  90%|█████████ | 9/10 [00:49<00:07,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_9.png: PickScore=20.9019, Adjusted=4.5096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PickScore: 100%|██████████| 10/10 [00:59<00:00,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_10.png: PickScore=20.5307, Adjusted=2.6535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "proc = AutoProcessor.from_pretrained(\"yuvalkirstain/PickScore_v1\")\n",
    "model = AutoModel.from_pretrained(\"yuvalkirstain/PickScore_v1\")\n",
    "\n",
    "for i in tqdm(range(1, 11), desc=\"Evaluating PickScore\"):\n",
    "    img = Image.open(f\"results/result_{i}.png\").convert(\"RGB\")\n",
    "    inputs = proc(text=[\"a hand\"], images=[img], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        score = model(**inputs).logits_per_image.squeeze().item()\n",
    "    adjusted_score = (score - 20) * 3\n",
    "    print(f\"result_{i}.png: PickScore={score:.4f}, Adjusted={adjusted_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04be740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PickScore: 19.453384399414062\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"results/result_5.png\").convert(\"RGB\")\n",
    "inputs = proc(text=[\"a hand\"], images=[img], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    score = model(**inputs).logits_per_image.squeeze().item()\n",
    "print(\"PickScore:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19cbe1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunilmadhow/anaconda3/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:899: FutureWarning: `callback` is deprecated and will be removed in version 1.0.0. Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\n",
      "  deprecate(\n",
      "/Users/sunilmadhow/anaconda3/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:905: FutureWarning: `callback_steps` is deprecated and will be removed in version 1.0.0. Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\n",
      "  deprecate(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b02e9fad3043a99165e4d5de7ac40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def collect_latents(pipe, step, timestep, callback_kwargs):\n",
    "    latents = callback_kwargs[\"latents\"]\n",
    "    t = int(timestep)\n",
    "    with torch.no_grad():\n",
    "        # Cast to float32 for training stability\n",
    "        xt = latents.detach().float().cpu()\n",
    "    latents_buffer.append((t, xt))  # you'll add your label y = exp(r) separately\n",
    "    callback_kwargs[\"latents\"] = latents\n",
    "    return callback_kwargs\n",
    "\n",
    "\n",
    "\n",
    "latents_buffer = []\n",
    "images = pipe(\n",
    "    \"A hand\",\n",
    "    num_inference_steps=25,\n",
    "    num_images_per_prompt=10,\n",
    "    callback_on_step_end=collect_latents,   # new preferred key\n",
    ").images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1585f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(images):\n",
    "    img.save(f\"results_/result_{i+1}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767a2a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
